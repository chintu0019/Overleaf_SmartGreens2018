\section{The Proposed Solutions}
In this section, we describe the proposed analytics methods that aim to provide useful tools to solve the four major problems mentioned in the introduction: resource provision, problems diagnosis, understanding user satisfaction and user behaviours.

\subsection{Resource Provision}
Let us start with the left-most branch from diagram in Figure~\ref{fig:schema}. A typical problem on cloud-based environment is the networking capacity management, for example the Virtual Machine (VM) selections. A common problem experienced in data centers and utility clouds is lack of knowledge about the mappings of the services being run by or offered to external users to the sets of virtual machines (VMs) that implement them~\cite{Wang2011}. In order to solve this problem,  propose a system integrating monitoring with analytics, termed Monalytics, which can capture, aggregate, and incrementally analyze data on- demand and in real-time, only where (i.e., in situ) and to the extents needed by intended management actions. This is done by collecting CPU utilizations of VMs on each host. A central node gathering the data runs the lightweight clustering algorithms described in [9]. Its output is a list of potential VM ensembles, i.e., VMs that probably communicate regularly. 


The new overlay gathers IP package statistics from the VMs and analyzes total network traffic by using Top-K flow analysis [26]. The analysis finds the k flows that most contribute to the traffic between any two VMs and their sizes. It eliminates any member of the ensemble with coincidental correlations in terms of CPU usage, and provides the flow data needed to better assess the cost-benefits derived from VM migration.

 
 
 The restrictions caused by VM capacity predictions have limited the performances of most current cloud-based network management solutions~\cite{Sun2016}. It means that it is hard to obtain the values of the inputs for most existing planning approaches, even though most optimized techniques highly rely on the networking states [7], [8]. 
 
 We aim to propose a feasible method that efficiently predicts VM states that can be used as the inputs of the existing networking capacity management techniques. The outcomes of this work are significant for current enterprises that implement cloud-based applications by providing a panoramic view of the networking capacity management in different application scenarios.
 
 



\subsection{Problem Diagnosis}
With the increasing scale and complexity of software
systems, it has become more and more difficult for system operators to understand the behaviors of software systems for tasks such as system problem diagnosis. For example, system operators need to understand system behaviors to figure out why a software system is in the current status. With such an understanding, they can choose the right operations to achieve the desired goal. System behaviors include a series of actions executed by the system and the corresponding changes in the system states. Although operators usually investigate a system starting from a specific state of interest, e.g., a hang state or failure state, contextual information for reaching that state is critical for identifying why the system runs in that state. Such contextual information includes how previous actions are executed by the system, what the historical system states are before running into the state of interest, what the input data is, etc.


Today more and more (monolithic) applications are decomposed into smaller components which are then executed as services on virtualized platforms connected via network communication and orchestrated to deliver the desired functionality. While the foundations for decomposing, executing and orchestrating were well settled over the past decade, allocating the needed resources for and steering the execution of components to deliver the required Quality of Service (QoS) is an active area of research. A critical aspect of steering complex service-based applications on virtualized platforms is effective, non-intrusive, low-footprint monitoring of key performance indicators at different provisioning tiers typically Infrastructure- as-a-Service, Platform-as-a-Service and Software-as- a-Service. These key performance indicators are assessed to verify that a Service Level Agreement (SLA) between a customer and a provider is met. Ideally, the assessment goes beyond simply detecting violations of the agreed terms, but tries to predict and preempt potential violations. The provider enacts counter-measures to prevent or resolve the violation if it does occur. Deriving effective counter-measures requires precise monitoring information spanning multiple tiers of the virtualized platform and analysis of monitoring data to identify the root cause(s) of performance problems.

Monitoring systems have been used over decades in different computing paradigms. Monitoring solutions for previous computing paradigms pose significant limitations for their widespread adoption in large scale, virtual platforms. The major obstacles with these monitoring techniques are, their high performance overhead, reliability, isolation, limited scalability, reliance on proprietary protocols and technologies.

Common performance diagnosis procedures depend on system administratorâ€™s domain knowledge and associated performance best practices. This procedure is labor intensive, error prone, and not feasible for virtual platforms. The prior art on detecting and diagnosing faults in computing systems can be reviewed in (Appleby et al., 2001) (Molenkamp, 2002) (Agarwal et al., 2004) (Chen et al., 2002)(Barham et al., 2003). These methods do not consider virtualization technologies and are inappropriate for rapidly changing, large scale virtual platforms that by very nature require effective automated techniques for QoS fault diagnosis.

Our research motivations are to study the effectiveness and practicality of different techniques for performance problem diagnosis and SLA based re- source management of virtual platforms. 


\subsection{Understanding User Behavior}


\subsection{Understanding User Satisfaction}
There are two main categories of challenges to overcome in or-
der to achieve the stated objectives. The first category is rooted from the characteristics of the data being analyzed with analytic technologies. Data scale. Typical data in software analytics is of large scale,
e.g., due to the large scale of software being developed and the large size of software development teams. Some tasks require to analyze division-wide or even company-wide code bases, which are far be- yond the scope of a single code base (e.g., when conducting code- clone detection [2]). Some tasks require to analyze a large quantity of (likely noisy) data samples within or beyond a single code base (e.g., when conducting runtime-trace analysis [3]). Although lack- ing data samples may not be an issue in this context of machine learning, the large scale of data poses challenges for data process- ing and analysis, including learning-algorithm design and system building. Data complexity. Typical data in software analytics is of high
complexity, which is partly due to the high complexity of software being developed. For example, runtime traces fromdistributed sys- tems [3] need to be correlated, while traces frommultiple threads [7] need to be split. System logs [3] include unstructured textual in- formation. There could be high dependencies across traces and noises among traces. In addition, real-world usage data produced from in-field operations offers substantial opportunities for various tasks such as debugging (e.g., those assisted by the Microsoft Er- ror Reporting system [4]). In addition to high complexity, such data is typically distributed and often partial (e.g., collected with sampling-based techniques to reduce runtime overhead). All these characteristics pose challenges for analytic technologies such as machine learning. The second category is rooted from the characteristics of the
tasks being assisted by software analytics. Focus on ultimate tasks being assisted. Among tasks assisted
by software analytics, some tasks are intermediate tasks and some are ultimate tasks. Usually intermediate tasks produce information toward solving ultimate tasks. For example, code-clone detection is considered as an intermediate task, which produces information towards refactoring and defect detection that are ultimate tasks.

Such focus on ultimate tasks requires the mandatory inclusion of the phase of deployment and feedback gathering in the life cycle of a software analytic project. Unlike most previous research on code-clone detection, we should not stop at measuring the preci- sion and recall of detected clones; rather, we should push further to accomplish that the detected clones could effectively help address ultimate tasks such as refactoring and defect detection, and should measure such benefits in evaluations. Engagement of customers during the development process of a
software analytic project. It is well recognized that engaging cus- tomers is a challenging task especially in the context of software engineering tools. Customers may have resistance to proposed changes (due to analytic-tool adoption) on their existing way of carrying out a task. In addition, due to tight development sched- ule, they may not be able to invest time on gaining understanding of the best/worst scenarios for applying an analytic tool. However, developing a software analytic project typically needs the engage- ment of customers in iterations of the four phases in the project life cycle, e.g., to get better understanding on the tasks and domain knowledge. Among the phases, especially the phase of deployment and feedback gathering, it is crucial for the produced analytic tools to have good usability, e.g., providing effective visualization and manipulation of analysis results.


